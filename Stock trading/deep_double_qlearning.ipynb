{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115b2e7d",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf # # python3 -m pip install tensorflow\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime\n",
    "import yfinance as yf # Must be v0.1.83: https://github.com/ranaroussi/yfinance/issues/1484\n",
    "import talib as ta\n",
    "import pickle\n",
    "import pyfolio as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a9ec9",
   "metadata": {},
   "source": [
    "# Build price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=\"1982-04-09\"\n",
    "start = \"1993-01-29\"\n",
    "end=\"2023-04-09\"\n",
    "\n",
    "aapl_data = yf.download(tickers=\"AAPL\",interval='1d', start=start, end=end , auto_adjust=True)\n",
    "spy_data = yf.download(tickers=\"SPY\", interval='1d', start=start, end=end, auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c76499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59200892",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = pd.DataFrame(index=aapl_data.index)\n",
    "\n",
    "# spy_data = tmp.merge(spy_data, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# spy_data = spy_data.fillna(method='ffill')\n",
    "# spy_data = spy_data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825669c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "aapl_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a364c8",
   "metadata": {},
   "source": [
    "### Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = .9\n",
    "split = int(t*len(aapl_data))\n",
    "\n",
    "train_aapl = aapl_data[:split]\n",
    "test_aapl = aapl_data[split:]\n",
    "\n",
    "train_spy = spy_data[:split]\n",
    "test_spy = spy_data[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369736c",
   "metadata": {},
   "source": [
    "# Design reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define profit and loss to incluude transaction cost and commissions\n",
    "def get_pnl(entry, curr_price, position):\n",
    "    # Transaction cost and commissions\n",
    "    tc = 0.001\n",
    "    return (curr_price*(1-tc) - entry*(1+tc))/entry*(1+tc)*position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf843d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_categorical_pnl(entry, curr_price, position):\n",
    "    '''Sign of pnl'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "    return np.sign(pnl)\n",
    "\n",
    "def reward_exponential_pnl(entry, curr_price, position):\n",
    "    '''Exponentual percentage pnl'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "    return np.exp(pnl)\n",
    "\n",
    "def reward_positive_categorical_pnl(entry, curr_price, position):\n",
    "    '''1 for win, 0 for loss'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "    if pnl >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_pnl(entry, curr_price, position, tc=0.001):\n",
    "#     return (curr_price*(1-tc) - entry*(1+tc))/entry*(1+tc)*position\n",
    "\n",
    "# def reward_sharpe_ratio(entry, curr_price, position, returns_window):\n",
    "#     pnl = get_pnl(entry, curr_price, position)\n",
    "#     returns_window.append(pnl)\n",
    "    \n",
    "#     if len(returns_window) > 1:\n",
    "#         excess_returns = np.array(returns_window) - 0.0  # Assuming a risk-free rate of 0.0\n",
    "#         sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns)\n",
    "#     else:\n",
    "#         sharpe_ratio = 0.0\n",
    "        \n",
    "#     return sharpe_ratio\n",
    "\n",
    "# # Example usage\n",
    "# returns_window = []\n",
    "# entry = 1000\n",
    "# curr_price = 102\n",
    "# position = -1\n",
    "# reward = reward_sharpe_ratio(entry, curr_price, position, returns_window)\n",
    "# print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a928ab2",
   "metadata": {},
   "source": [
    "# Build the RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f8871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25561479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "\n",
    "    def __init__(self, bars1,bars2, reward_function, lkbk=14,  init_idx=None):\n",
    "\n",
    "        # Initilaise lookback period for the calculation of technical indicators\n",
    "        self.lkbk = lkbk\n",
    "        # Intialise length of each trade\n",
    "        self.trade_len = 0\n",
    "        # Initialise 1 day frequency data (just 2 price series (aapl and spy))\n",
    "        self.bars1 = bars1\n",
    "        self.bars2 = bars2\n",
    "        # Initialise when game is over to update the state, position and calculate reward\n",
    "        self.is_over = False\n",
    "        # Intialise reward to store the value of reward\n",
    "        self.reward = 0\n",
    "        # Define pnl_sum to calculate the pnl when all episodes are complete.\n",
    "        self.pnl_sum = 0\n",
    "        # Supply a starting index which indicates a position in our price dataframe\n",
    "        # and denotes the point at which the game starts\n",
    "        self.init_idx = init_idx\n",
    "        # Instantiate reward function\n",
    "        self.reward_function = reward_function\n",
    "        # When game is over, reset all state values\n",
    "        self.reset()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _update_position(self, action):\n",
    "        '''This is where we update our position'''\n",
    "\n",
    "        # If the action is zero or hold, do nothing (action==0 means hold)\n",
    "        if action == 0:\n",
    "            pass\n",
    "\n",
    "        elif action == 2:\n",
    "            \"\"\"Enter a long or exit a short position\"\"\"\n",
    "\n",
    "            # Current position (long) same as the action (buy), do nothing (action==2 means buy)\n",
    "            if self.position == 1:\n",
    "                pass\n",
    "\n",
    "            # No current position, and action is buy, update the position to indicate buy\n",
    "            elif self.position == 0:\n",
    "                self.position = 1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "\n",
    "            # Current postion (short) is different than the action (buy), end the game\n",
    "            elif self.position == -1:\n",
    "                self.is_over = True\n",
    "\n",
    "        elif action == 1:\n",
    "            \"\"\"Enter a short or exit a long position\"\"\"\n",
    "\n",
    "            # Current position (short) same as the action (sell), do nothing (action==1 means sell)\n",
    "            if self.position == -1:\n",
    "                pass\n",
    "\n",
    "            # No current position, and action is sell, update the position to indicate sell\n",
    "            elif self.position == 0:\n",
    "                self.position = -1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "\n",
    "            # Current postion (long) is different than the action (sell), end the game\n",
    "            elif self.position == 1:\n",
    "                self.is_over = True\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"Here we calculate the reward when the game is finished.\n",
    "        \"\"\"\n",
    "        if self.is_over:\n",
    "            self.reward = self.reward_function(\n",
    "                self.entry, self.curr_price, self.position)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _get_last_N_timebars(self):\n",
    "        '''This function gets the timebars 1 day resolution based on the lookback specified.'''\n",
    "\n",
    "        '''Width of the 1d'''\n",
    "        self.wdw1d = np.ceil(self.lkbk*14)\n",
    "\n",
    "        '''Getting candlesticks before current time'''\n",
    "        self.last1 = self.bars1[self.curr_time -\n",
    "                                  timedelta(self.wdw1d):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last2 = self.bars2[self.curr_time -\n",
    "                                  timedelta(self.wdw1d):self.curr_time].iloc[-self.lkbk:]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _assemble_state(self):\n",
    "        self._get_last_N_timebars()\n",
    "\n",
    "        \"\"\"Adding State Variables\"\"\"\n",
    "        self.state = np.array([])\n",
    "\n",
    "        \"\"\"Adding candlesticks\"\"\"\n",
    "        def get_normalised_bars_array(bars):\n",
    "            bars = bars.iloc[-10:].values.flatten()\n",
    "            bars = (bars-np.mean(bars))/np.std(bars)\n",
    "            return bars\n",
    "\n",
    "        self.state = np.append(self.state, get_normalised_bars_array(self.last1))\n",
    "        self.state = np.append(self.state, get_normalised_bars_array(self.last2))\n",
    "\n",
    "\n",
    "        \"\"\"\" Adding technical indicators\"\"\"\n",
    "        \n",
    "    def get_technical_indicators(bars):\n",
    "\n",
    "        #--------------\n",
    "\n",
    "        def get_psma(self, prices, window):\n",
    "            rm = prices.rolling(window).mean()\n",
    "            psma = prices.divide(rm, axis=0) - 1\n",
    "            return psma \n",
    "\n",
    "        def get_bb(self, prices, window):\n",
    "            rm = prices.rolling(window).mean()\n",
    "            rstd = prices.rolling(window).std()\n",
    "            bbp = (prices - rm) / 2 * rstd\n",
    "            return bbp\n",
    "\n",
    "        def get_pema(self, prices, window):\n",
    "            ema = prices.ewm(window).mean()\n",
    "            pema = prices.divide(ema, axis=0) - 1\n",
    "            return pema\n",
    "\n",
    "        #--------------\n",
    "\n",
    "        \"\"\"Relative difference two moving averages\"\"\"\n",
    "        sma1 = ta.SMA(bars.Close, self.lkbk-1)[-1]\n",
    "        sma2 = ta.SMA(bars.Close, self.lkbk-8)[-1]\n",
    "        \"\"\"Relative momentums\"\"\"\n",
    "        mom10 = ta.MOM(bars.Close, 10)[-1]\n",
    "        mom20 = ta.MOM(bars.Close, 20)[-1]\n",
    "        mom30 = ta.MOM(bars.Close, 30)[-1]\n",
    "        \"\"\"Relative returns\"\"\"\n",
    "        ret1 = bars['Close'].pct_change()\n",
    "        ret5 = ret1.rolling(5).sum()[-1]\n",
    "        ret10 = ret1.rolling(10).sum()[-1]\n",
    "        ret20 = ret1.rolling(20).sum()[-1]\n",
    "        ret40 = ret1.rolling(40).sum()[-1]    \n",
    "        \"\"\"hybrid indicators\"\"\"\n",
    "        bb = get_bb(bars.Close, window=self.lkbk)[-1]\n",
    "        psma = get_psma(bars,Close, window=self.lkbk)[-1]\n",
    "        pema = get_pema(bars.Close, window=self.lkbk)[-1]\n",
    "\n",
    "        tech_ind1 = np.append(tech_ind1, (sma1-sma2)/sma2)\n",
    "        tech_ind1 = np.append(tech_ind1, mom10)\n",
    "        tech_ind1 = np.append(tech_ind1, mom20)\n",
    "        tech_ind1 = np.append(tech_ind1, mom30)\n",
    "        tech_ind1 = np.append(tech_ind1, ret5)\n",
    "        tech_ind1 = np.append(tech_ind1, ret10)\n",
    "        tech_ind1 = np.append(tech_ind1, ret20)\n",
    "        tech_ind1 = np.append(tech_ind1, ret40)\n",
    "        tech_ind1 = np.append(tech_ind1, bb)\n",
    "        tech_ind1 = np.append(tech_ind1, psma)\n",
    "        tech_ind1 = np.append(tech_ind1, pema)\n",
    "\n",
    "        # technical indicators\n",
    "        functions = [ta.RSI, ta.SAR, ta.ADX, ta.NATR, ta.AROONOSC,\n",
    "                     ta.BOP,ta.BETA, ta.STDDEV, ta.OBV]\n",
    "\n",
    "\n",
    "        # Loop over the functions and append the last value of each indicator to tech_ind2\n",
    "        tech_ind2 = np.array([])\n",
    "\n",
    "        for function in functions:\n",
    "            try:\n",
    "                if function in [ta.RSI]:\n",
    "                    indicator_values = function(bars.Close.shift(-1), timeperiod=self.lkbk)\n",
    "\n",
    "                elif function in [ta.STDDEV]:\n",
    "                    indicator_values = function(bars.Close)\n",
    "\n",
    "                elif function in [ta.ADX]:\n",
    "                    indicator_values = function(bars.High.shift(1), bars.Low.shift(1),\n",
    "                                                bars.Open, timeperiod=self.lkbk)\n",
    "\n",
    "                elif function in [ta.SAR]:\n",
    "                    indicator_values = function(bars.High.shift(1), bars.Low.shift(1),\n",
    "                                                0.2, 0.2)\n",
    "\n",
    "                elif function in [ta.NATR]:\n",
    "                    indicator_values = function(bars.Low, bars.High, bars.Close,\n",
    "                                                timeperiod=self.lkbk)\n",
    "\n",
    "                elif function in [ta.AROONOSC]:\n",
    "                    indicator_values = function(bars.High, bars.Low,\n",
    "                                                timeperiod = self.lkbk-3)\n",
    "\n",
    "                elif function in [ta.BETA]:\n",
    "                    indicator_values = function(bars.High, bars.Low)\n",
    "\n",
    "                elif function in [ta.BOP]:\n",
    "                    indicator_values = function(bars.Open, bars.High, \n",
    "                                                bars.Low, bars.Close)\n",
    "\n",
    "                else:\n",
    "                    indicator_values = function(bars.Close, bars.Volume)\n",
    "\n",
    "                last_value = indicator_values[-1]\n",
    "                tech_ind2 = np.append(tech_ind2, last_value)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying {function.__name__}: {e}\")\n",
    "\n",
    "\n",
    "        tech_ind = np.concatenate((tech_ind1, tech_ind2), axis=0).tolist()\n",
    "\n",
    "        return tech_ind\n",
    "\n",
    "        self.state = np.append(self.state, get_technical_indicators(self.last1))\n",
    "        self.state = np.append(self.state, get_technical_indicators(self.last2))\n",
    "\n",
    "        \"\"\"Extract features from Dates\"\"\"\n",
    "        self.curr_time = self.bars1.index[self.curr_idx]\n",
    "        _day_of_week = self.curr_time.weekday()/6\n",
    "        _week = int(self.curr_time.strftime(\"%U\"))/51 # 52 weeks with index starting from 0\n",
    "        _month_of_year = self.curr_time.month/11\n",
    "        _year = self.curr_time.year/30 # thirty years of data\n",
    "\n",
    "        self.state = np.append(self.state, self._day_of_week)\n",
    "        self.state = np.append(self.state, self._week)\n",
    "        self.state = np.append(self.state, self._month_of_year)\n",
    "        self.state = np.append(self.state, self._year)\n",
    "        \"\"\"it is important to add position\"\"\"\n",
    "        self.state = np.append(self.state, self.position)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"This function returns the state of the system.\n",
    "        Returns:\n",
    "            self.state: the state including candlestick bars, indicators, time signatures and position.\n",
    "        \"\"\"\n",
    "        # Assemble new state\n",
    "        self._assemble_state()\n",
    "        return np.array([self.state])\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "    def get_position(self, action):\n",
    "        \"\"\"\n",
    "        Get the position in the market based on the action suggested by the RL agent.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The action suggested by the RL agent. 0 means hold, 1 means sell, and 2 means buy.\n",
    "        \n",
    "        Returns:\n",
    "            int: The current position in the market. 0 means no position, 1 means long, and -1 means short.\n",
    "        \"\"\"\n",
    "        if action == 0:  # Hold\n",
    "            return self.position\n",
    "        elif action == 2:  # Buy\n",
    "            if self.position == 1:  # If the current position is already long, do nothing.\n",
    "                return self.position\n",
    "            elif self.position == 0:  # If there's no current position, update the position to indicate buy.\n",
    "                return 1\n",
    "            elif self.position == -1:  # If the current position is short, end the game.\n",
    "                self.is_over = True\n",
    "                return self.position\n",
    "        elif action == 1:  # Sell\n",
    "            if self.position == -1:  # If the current position is already short, do nothing.\n",
    "                return self.position\n",
    "            elif self.position == 0:  # If there's no current position, update the position to indicate sell.\n",
    "                return -1\n",
    "            elif self.position == 1:  # If the current position is long, end the game.\n",
    "                self.is_over = True\n",
    "                return self.position\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "    def act(self, action):\n",
    "        \"\"\"This is the point where the game interacts with the trading\n",
    "        algo. It returns value of reward when game is over.\n",
    "        \"\"\"\n",
    "        # note that here we're only using the aapl price and not spy which was only used to generate technicals\n",
    "        self.curr_time = self.bars1.index[self.curr_idx]\n",
    "        self.curr_price = self.bars1['Close'][self.curr_idx]\n",
    "\n",
    "        self._update_position(action)\n",
    "\n",
    "        # Unrealized or realized pnl. This is different from pnl in reward method which is only realized pnl.\n",
    "        self.pnl = (-self.entry + self.curr_price)*self.position/self.entry\n",
    "\n",
    "        self._get_reward()\n",
    "        if self.is_over:\n",
    "            self.trade_len = self.curr_idx - self.start_idx\n",
    "\n",
    "        return self.is_over, self.reward\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        \"\"\"Resetting the system for each new trading game.\n",
    "        Here, we also resample the bars for 1d.\n",
    "        Ideally, we should do this on every update but this will take very long.\n",
    "        \"\"\"\n",
    "        self.pnl = 0\n",
    "        self.entry = 0\n",
    "        self._day_of_week = 0\n",
    "        self._week = 0\n",
    "        self._month_of_year = 0\n",
    "        self._year = 0\n",
    "        self.curr_idx = self.init_idx\n",
    "        self.start_idx = self.curr_idx\n",
    "        self.curr_time = self.bars1.index[self.curr_idx]\n",
    "        self._get_last_N_timebars()\n",
    "        self.position = 0\n",
    "        self.act(0)\n",
    "        self.state = []\n",
    "        self._assemble_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a8919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da0eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50af3a77",
   "metadata": {},
   "source": [
    "## Defining the method to create the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df079416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_net(env, rl_config):\n",
    "#     \"\"\"\n",
    "#     This initialises the RL run by\n",
    "#     creating two new predictive neural network\n",
    "#     Args:\n",
    "#         env:\n",
    "#     Returns:\n",
    "#         modelQ: the neural network\n",
    "#         modelR: the neural network\n",
    "\n",
    "#     \"\"\"\n",
    "#     hidden_size = len(env.state)*rl_config['HIDDEN_MULT']\n",
    "#     modelQ = Sequential()\n",
    "#     modelQ.add(LSTM(len(env.state), input_shape=(\n",
    "#         len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(LSTM(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "#     modelQ.compile(Adam(lr=rl_config['LEARNING_RATE']),\n",
    "#                    loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "#     modelR = Sequential()\n",
    "#     modelR.add(LSTM(len(env.state), input_shape=(\n",
    "#         len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(LSTM(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "#     modelR.compile(Adam(learning_rate=rl_config['LEARNING_RATE']),\n",
    "#                    loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "#     return modelQ, modelR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import LSTM, Dense\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import SGD\n",
    "\n",
    "# def init_net(env, rl_config):\n",
    "#     \"\"\"\n",
    "#     This initialises the RL run by\n",
    "#     creating two new predictive neural network\n",
    "#     Args:\n",
    "#         env:\n",
    "#     Returns:\n",
    "#         modelQ: the neural network\n",
    "#         modelR: the neural network\n",
    "\n",
    "#     \"\"\"\n",
    "#     hidden_size = len(env.state)*rl_config['HIDDEN_MULT']\n",
    "#     timesteps = 1  # You may need to adjust this depending on the input shape of your environment's state\n",
    "\n",
    "#     modelQ = Sequential()\n",
    "#     modelQ.add(LSTM(hidden_size, input_shape=(timesteps, len(env.state)), activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "#     modelQ.compile(SGD(lr=rl_config['LEARNING_RATE']), loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "#     modelR = Sequential()\n",
    "#     modelR.add(LSTM(hidden_size, input_shape=(timesteps, len(env.state)), activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "#     modelR.compile(SGD(lr=rl_config['LEARNING_RATE']), loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "#     return modelQ, modelR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f008db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def init_net(env, rl_config):\n",
    "    \"\"\"\n",
    "    This initialises the RL run by\n",
    "    creating two new predictive neural network\n",
    "    Args:\n",
    "        env:\n",
    "    Returns:\n",
    "        modelQ: the neural network\n",
    "        modelR: the neural network\n",
    "\n",
    "    \"\"\"\n",
    "    dropout_rate = 0.5  # adjust the dropout rate as needed\n",
    "    hidden_size = len(env.state) * rl_config['HIDDEN_MULT']\n",
    "\n",
    "    modelQ = Sequential()\n",
    "    modelQ.add(Dense(len(env.state), input_shape=(\n",
    "        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dropout(dropout_rate))\n",
    "    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dropout(dropout_rate))\n",
    "    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dropout(dropout_rate))\n",
    "    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dropout(dropout_rate))\n",
    "    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dropout(dropout_rate))\n",
    "    modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "    modelQ.compile(Adam(lr=rl_config['LEARNING_RATE']),\n",
    "                   loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "    modelR = Sequential()\n",
    "    modelR.add(Dense(len(env.state), input_shape=(\n",
    "        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dropout(dropout_rate))\n",
    "    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dropout(dropout_rate))\n",
    "    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dropout(dropout_rate))\n",
    "    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dropout(dropout_rate))\n",
    "    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dropout(dropout_rate))\n",
    "    modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "    modelR.compile(Adam(lr=rl_config['LEARNING_RATE']),\n",
    "                   loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "    return modelQ, modelR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2adb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_net(env, rl_config):\n",
    "#     \"\"\"\n",
    "#     This initialises the RL run by\n",
    "#     creating two new predictive neural network\n",
    "#     Args:\n",
    "#         env:\n",
    "#     Returns:\n",
    "#         modelQ: the neural network\n",
    "#         modelR: the neural network\n",
    "\n",
    "#     \"\"\"\n",
    "#     hidden_size = len(env.state)*rl_config['HIDDEN_MULT']\n",
    "    \n",
    "#     modelQ = Sequential()\n",
    "#     modelQ.add(Dense(len(env.state), input_shape=(len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "#     modelQ.compile(Adam(lr=rl_config['LEARNING_RATE']), loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "#     modelR = Sequential()\n",
    "#     modelR.add(Dense(len(env.state), input_shape=(len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "#     modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "#     modelR.compile(Adam(lr=rl_config['LEARNING_RATE']), loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "#     return modelQ, modelR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066957e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars1 = train_aapl\n",
    "bars2 = train_spy\n",
    "# Create Game class environment\n",
    "env = Game(bars1,bars2, reward_exponential_pnl,lkbk=14,  init_idx=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ccf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.act(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_position(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae86e3",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    '''This class calculates the Q-Table.\n",
    "    It gathers memory from previous experience and \n",
    "    creates a Q-Table with states and rewards for each\n",
    "    action using the NN. At the end of the game the reward\n",
    "    is calculated from the reward function. \n",
    "    The weights in the NN are constantly updated with new\n",
    "    batch of experience. \n",
    "    This is the heart of the RL algorithm.\n",
    "    Args:\n",
    "        state_tp1: the state at time t+1\n",
    "        state_t: the state at time t\n",
    "        action_t: int {0..2} hold, sell, buy taken at state_t \n",
    "        Q_sa: float, the reward for state_tp1\n",
    "        reward_t: the reward for state_t\n",
    "        self.memory: list of state_t, action_t and reward_t at time t as well as state_tp1\n",
    "        targets: array(float) Nx2, weight of each action\n",
    "        inputs: an array with scrambled states at different times\n",
    "        targets: Nx3 array of weights for each action for scrambled input states\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_memory, discount):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''Add states to time t and t+1 as well as  to memory'''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def process(self, modelQ, modelR, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = modelQ.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "\n",
    "        \"\"\"Initialise input and target arrays\"\"\"\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "\n",
    "        # Option 1\n",
    "        \"\"\"\n",
    "        Random Sampling for loop:\n",
    "        Step randomly through different places in the memory\n",
    "        and scramble them into a new input array (inputs) with the\n",
    "        length of the pre-defined batch size\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):    \n",
    "            # Obtain the parameters for Bellman from memory,\n",
    "            # S.A.R.S: state, action, reward, new state\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            # Boolean flag to check if the game is over\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = state_t    \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Option 2\n",
    "\n",
    "        \"\"\"\n",
    "        Recency Sampling for loop:\n",
    "        Select sequentially from the most recent memories. The number of memories fetched is\n",
    "        determined by the batch size.\n",
    "        \n",
    "        for i, idx in enumerate(np.arange(-inputs.shape[0],0))\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Implementing the recency sampling loop\n",
    "        for i, idx in enumerate(np.arange(-inputs.shape[0], 0)):\n",
    "            \"\"\"Obtain the parameters for Bellman from memory,\n",
    "            S.A.R.S: state, action, reward, new state.\"\"\"\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = state_t\n",
    "\n",
    "            \"\"\"Calculate the targets for the state at time t\"\"\"\n",
    "            targets[i] = modelR.predict(state_t)[0]\n",
    "\n",
    "            \"\"\"Calculate the reward at time t+1 for action at time t\"\"\"\n",
    "            Q_sa = np.max(modelQ.predict(state_tp1)[0])\n",
    "\n",
    "            if game_over:\n",
    "                \"\"\"When game is over we have a definite reward\"\"\"\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                \"\"\"\n",
    "                Update the part of the target for which action_t occurred to new value\n",
    "                Q_new(s,a) = reward_t + gamma * max_a' Q(s', a')\n",
    "                \"\"\"\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9547e83",
   "metadata": {},
   "source": [
    "# Backtest function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5fa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(bars1,bars2, rl_config):\n",
    "    \"\"\"\n",
    "    Function to run the RL model on the passed price data\n",
    "    \"\"\"\n",
    "\n",
    "    pnls = []\n",
    "    trade_logs = pd.DataFrame()\n",
    "    episode = 0\n",
    "\n",
    "    \"\"\"Initialise a NN and a set up initial game parameters and experience replay\"\"\"\n",
    "    env = Game(bars1,bars2, rl_config['RF'],\n",
    "               lkbk=rl_config['LKBK'], init_idx=rl_config['START_IDX'])\n",
    "    q_network, r_network = init_net(env, rl_config)\n",
    "    exp_replay = ExperienceReplay(\n",
    "        max_memory=rl_config['MAX_MEM'], discount=rl_config['DISCOUNT_RATE'])\n",
    "    \n",
    "    \"\"\"Preloading the model weights\"\"\"\n",
    "    if rl_config['PRELOAD']:\n",
    "        q_network.load_weights(rl_config['WEIGHTS_FILE'])\n",
    "        r_network.load_weights(rl_config['WEIGHTS_FILE'])\n",
    "        exp_replay.memory = pickle.load(open(rl_config['REPLAY_FILE'], 'rb'))\n",
    "\n",
    "    r_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    \"\"\"Loop that steps through one trade (game) at a time\"\"\"\n",
    "    while True:\n",
    "        \"\"\"Stop the algo when end is near to avoid exception\"\"\"\n",
    "        if env.curr_idx >= len(bars1)-1:\n",
    "            break\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        \"\"\"Initialise a new game\"\"\"\n",
    "        env = Game(bars1,bars2, rl_config['RF'],\n",
    "                   lkbk=rl_config['LKBK'], init_idx=env.curr_idx)\n",
    "        state_tp1 = env.get_state()\n",
    "\n",
    "        \"\"\"Calculate epsilon for exploration vs exploitation random action generator\"\"\"\n",
    "#         epsilon = rl_config['EPSILON']**(np.log10(episode)) + rl_config['EPS_MIN']\n",
    "        epsilon = rl_config['EPSILON']\n",
    "        epsilon_decay = 0.995       \n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(rl_config['EPS_MIN'], epsilon)\n",
    "\n",
    "\n",
    "        game_over = False\n",
    "        cnt = 0\n",
    "\n",
    "        \"\"\"Walkthrough time steps starting from the end of the last game\"\"\"\n",
    "        while not game_over:\n",
    "\n",
    "            if env.curr_idx >= len(bars1)-1:\n",
    "                break\n",
    "\n",
    "            cnt += 1\n",
    "            state_t = state_tp1\n",
    "\n",
    "            \"\"\"Generate a random action or through q_network\"\"\"\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, 3, size=1)[0]\n",
    "\n",
    "            else:\n",
    "                q = q_network.predict(state_t)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            \"\"\"Updating the Game\"\"\"\n",
    "            reward, game_over = env.act(action)\n",
    "\n",
    "            \"\"\"Updating trade/position logs\"\"\"\n",
    "            tl = [[env.curr_time, env.position, episode]]\n",
    "            if game_over:\n",
    "                tl = [[env.curr_time, 0, episode]]\n",
    "            trade_logs = trade_logs.append(tl)\n",
    "\n",
    "            \"\"\"Move to next time step\"\"\"\n",
    "            env.curr_idx += 1\n",
    "            state_tp1 = env.get_state()\n",
    "\n",
    "            \"\"\"Adding state to memory\"\"\"\n",
    "            exp_replay.remember([state_t, action, reward, state_tp1], game_over)\n",
    "\n",
    "            \"\"\"Creating a new Q-Table\"\"\"\n",
    "            inputs, targets = exp_replay.process(q_network, r_network, batch_size=rl_config['BATCH_SIZE'])\n",
    "            env.pnl_sum = sum(pnls)\n",
    "\n",
    "            \"\"\"Update the NN model with a new Q-Table\"\"\"\n",
    "            q_network.train_on_batch(inputs, targets)\n",
    "\n",
    "            if game_over and rl_config['UPDATE_QR']:\n",
    "                r_network.set_weights(q_network.get_weights())\n",
    "\n",
    "        pnls.append(env.pnl)\n",
    "\n",
    "        print(\"Trade {:03d} | pos {} | len {} | approx cum ret {:,.2f}% | trade ret {:,.2f}% | eps {:,.4f} | {} | {}\".format(\n",
    "            episode, env.position, env.trade_len, sum(pnls)*100, env.pnl*100, epsilon, env.curr_time, env.curr_idx))\n",
    "\n",
    "        if not episode % 10:\n",
    "            print('---saving weights, trade logs and replay buffer---')\n",
    "            r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n",
    "            trade_logs.to_pickle(rl_config['TRADE_FILE'])\n",
    "            pickle.dump(exp_replay.memory, open(\n",
    "                rl_config['REPLAY_FILE'], 'wb'))\n",
    "\n",
    "        if not episode % 15 and rl_config['TEST_MODE']:\n",
    "            print('\\n**********************************************\\nTest mode is on due to resource constraints and therefore stopped after 15 trades. \\nYou can trade on full dataset on your local computer and set TEST_MODE flag to False in rl_config dictionary. \\n**********************************************\\n')\n",
    "            break\n",
    "\n",
    "    print('---saving weights, trade logs and replay buffer---')\n",
    "    r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n",
    "    trade_logs.to_pickle(rl_config['TRADE_FILE'])\n",
    "    pickle.dump(exp_replay.memory, open(rl_config['REPLAY_FILE'], 'wb'))\n",
    "\n",
    "    print('***FINISHED***')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19961c3b",
   "metadata": {},
   "source": [
    "# RL configuration and running the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a26b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For running the RL model on the price data, you need to \n",
    "set the configuration parameters.\n",
    "These configuration parameters are hyperparameters for the \n",
    "RL model and the ANN used in it.\n",
    "\"\"\"\n",
    "rl_config = {\n",
    "\n",
    "    'LEARNING_RATE': 1.0e-6, \n",
    "    'LOSS_FUNCTION': 'mse',\n",
    "    'ACTIVATION_FUN': 'relu',\n",
    "    'NUM_ACTIONS': 3,\n",
    "    'HIDDEN_MULT': 2, # \n",
    "    'DISCOUNT_RATE': 0.9, \n",
    "    'LKBK': 14,\n",
    "    'BATCH_SIZE': 10, # \n",
    "    'MAX_MEM': 1000,  #  \n",
    "    'EPSILON': 1.0, # \n",
    "    'EPS_MIN': 0.01,   # \n",
    "    'START_IDX': 300,\n",
    "    'WEIGHTS_FILE': 'indicator_model_aapl_1.h5',\n",
    "    'TRADE_FILE': 'trade_logs_aapl_1.bz2',\n",
    "    'REPLAY_FILE': 'memory_aapl_1.bz2',\n",
    "    'RF': reward_categorical_pnl,  # You can change the reward function here\n",
    "    'TEST_MODE': False,\n",
    "    'PRELOAD': False,\n",
    "    'UPDATE_QR': True\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Run the RL model on the price data\n",
    "Note: To run in a local machine, please change the `TEST_MODE` to \n",
    "`False` in `rl_config`\n",
    "\"\"\"\n",
    "run(bars1,bars2, rl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f42aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('trade_logs_aapl_0.bz2').to_csv('trade_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb779863",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_config = {\n",
    "\n",
    "    'LEARNING_RATE': 1.0e-5, \n",
    "    'LOSS_FUNCTION': 'mse',\n",
    "    'ACTIVATION_FUN': 'relu',\n",
    "    'NUM_ACTIONS': 3,\n",
    "    'HIDDEN_MULT': 2, # \n",
    "    'DISCOUNT_RATE': 0.9, \n",
    "    'LKBK': 14,\n",
    "    'BATCH_SIZE': 5, # \n",
    "    'MAX_MEM': 1000,  #  \n",
    "    'EPSILON': 1., # \n",
    "    'EPS_MIN': 0.01,   # \n",
    "    'START_IDX': 300,\n",
    "    'WEIGHTS_FILE': 'indicator_model_aapl_2.h5',\n",
    "    'TRADE_FILE': 'trade_logs_aapl_2.bz2',\n",
    "    'REPLAY_FILE': 'memory_aapl_2.bz2',\n",
    "    'RF': reward_categorical_pnl,  # You can change the reward function here\n",
    "    'TEST_MODE': False,\n",
    "    'PRELOAD': False,\n",
    "    'UPDATE_QR': True\n",
    "}\n",
    "\n",
    "run(bars1,bars2, rl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7399e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_config = {\n",
    "\n",
    "    'LEARNING_RATE': 1.0e-5, \n",
    "    'LOSS_FUNCTION': 'mse',\n",
    "    'ACTIVATION_FUN': 'relu',\n",
    "    'NUM_ACTIONS': 3,\n",
    "    'HIDDEN_MULT': 2, # \n",
    "    'DISCOUNT_RATE': 0.9, \n",
    "    'LKBK': 14,\n",
    "    'BATCH_SIZE': 1, # \n",
    "    'MAX_MEM': 1000,  #  \n",
    "    'EPSILON': 1., # \n",
    "    'EPS_MIN': 0.001,   # \n",
    "    'START_IDX': 300,\n",
    "    'WEIGHTS_FILE': 'indicator_model_aapl_3.h5',\n",
    "    'TRADE_FILE': 'trade_logs_aapl_3.bz2',\n",
    "    'REPLAY_FILE': 'memory_aapl_3.bz2',\n",
    "    'RF': reward_categorical_pnl,  # You can change the reward function here\n",
    "    'TEST_MODE': False,\n",
    "    'PRELOAD': False,\n",
    "    'UPDATE_QR': True\n",
    "}\n",
    "\n",
    "run(bars1,bars2, rl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1508f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rl_config = {\n",
    "\n",
    "    'LEARNING_RATE': 1.0e-5, \n",
    "    'LOSS_FUNCTION': 'mse',\n",
    "    'ACTIVATION_FUN': 'relu',\n",
    "    'NUM_ACTIONS': 3,\n",
    "    'HIDDEN_MULT': 3, # \n",
    "    'DISCOUNT_RATE': 0.99, \n",
    "    'LKBK': 14,\n",
    "    'BATCH_SIZE': 1, # \n",
    "    'MAX_MEM': 1000,  #  \n",
    "    'EPSILON': 1., # \n",
    "    'EPS_MIN': 0.01,   # \n",
    "    'START_IDX': 300,\n",
    "    'WEIGHTS_FILE': 'indicator_model_aapl_4.h5',\n",
    "    'TRADE_FILE': 'trade_logs_aapl_4.bz2',\n",
    "    'REPLAY_FILE': 'memory_aapl_4.bz2',\n",
    "    'RF': reward_categorical_pnl,  # You can change the reward function here\n",
    "    'TEST_MODE': False,\n",
    "    'PRELOAD': False,\n",
    "    'UPDATE_QR': True\n",
    "}\n",
    "\n",
    "run(bars1,bars2, rl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_config = {\n",
    "\n",
    "    'LEARNING_RATE': 1.0e-6, \n",
    "    'LOSS_FUNCTION': 'mse',\n",
    "    'ACTIVATION_FUN': 'relu',\n",
    "    'NUM_ACTIONS': 3,\n",
    "    'HIDDEN_MULT': 3, # \n",
    "    'DISCOUNT_RATE': 0.9, \n",
    "    'LKBK': 14,\n",
    "    'BATCH_SIZE': 1, # \n",
    "    'MAX_MEM': 1000,  #  \n",
    "    'EPSILON': 1., # \n",
    "    'EPS_MIN': 0.0001,   # \n",
    "    'START_IDX': 300,\n",
    "    'WEIGHTS_FILE': 'indicator_model_aapl_5.h5',\n",
    "    'TRADE_FILE': 'trade_logs_aapl_5.bz2',\n",
    "    'REPLAY_FILE': 'memory_aapl_5.bz2',\n",
    "    'RF': reward_categorical_pnl,  # You can change the reward function here\n",
    "    'TEST_MODE': False,\n",
    "    'PRELOAD': False,\n",
    "    'UPDATE_QR': True\n",
    "}\n",
    "\n",
    "run(bars1,bars2, rl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba20d2",
   "metadata": {},
   "source": [
    "# Performance on the test set\n",
    "\n",
    "Here we need to load the weights of the best agents based on its performance and use that agent to  to make decisions (take actions) on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d28f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_config = {\n",
    "\n",
    "#     'LEARNING_RATE': 0.000001, \n",
    "#     'LOSS_FUNCTION': 'mse',\n",
    "#     'ACTIVATION_FUN': 'relu',\n",
    "#     'NUM_ACTIONS': 3,\n",
    "#     'HIDDEN_MULT': 3, # \n",
    "#     'DISCOUNT_RATE': 0.99, \n",
    "#     'LKBK': 14,\n",
    "#     'BATCH_SIZE': 10, # \n",
    "#     'MAX_MEM': 600,  #  \n",
    "#     'EPSILON': 1.0, # 0.0001\n",
    "#     'EPS_MIN': 0.1,   # 0.001\n",
    "#     'START_IDX': 300,\n",
    "#     'WEIGHTS_FILE': 'indicator_model_aapl_4.h5', # note that the weights obtained while training will be overwritten.\n",
    "#     'TRADE_FILE': 'trade_logs_aapl_4.bz2',\n",
    "#     'REPLAY_FILE': 'memory_aapl_4.bz2',\n",
    "#     'RF': reward_categorical_pnl,  # You can change the reward function here\n",
    "#     'TEST_MODE': False,\n",
    "#     'PRELOAD': True, # here we're loading weights from the training phase\n",
    "#     'UPDATE_QR': True\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053219",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(test_aapl,test_spy, rl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e6a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
